{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generare i JSON per le mappe: tipo 1, conto dei residenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiettivo di questo notebook è di preparare i dati per  poi generare i JSON pronti per alimentare le mappe del progetto. Cominciamo con le mappe del tipo 1, che contano semplicemente i residenti. La spiegazione è [sul wiki](https://github.com/spaghetti-open-data/-milano-melting-pot/wiki/2.-Metodologia-e-workflow).\n",
    "\n",
    "Anzitutto verifichiamo i dati sui residenti. Il file JSON sembra corrotto. Controlliamo il CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136906\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "dirPath = '/Users/albertocottica/Documents/GitHub/-milano-melting-pot/dati/'\n",
    "data = [] # we know from JSON that \n",
    "with open(dirPath + 'ds27_pop_sto_quartiere.csv') as datafile:\n",
    "    reader = csv.DictReader(datafile, delimiter=';')\n",
    "    for row in reader:\n",
    "        drow = dict(row)\n",
    "        drow['Anno'] = drow.pop('\\ufeffAnno') # questo è per togliere spazzatura che confonde il nome del campo\n",
    "        data.append(drow)\n",
    "print (len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IdNil': '1', 'Nil': 'Duomo', 'Eta': '0', 'Genere': 'Femmine', 'Cittadinanza': 'Cinese, Rep. Popolare', 'Residenti': '1', 'Anno': '1999'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Duomo', 'Brera', 'Giardini Porta Venezia', 'Guastalla', 'Vigentina', 'Ticinese', 'Magenta - S. Vittore', 'Parco Sempione', 'Garibaldi Repubblica', 'Centrale', 'Isola', 'Maciachini - Maggiolina', 'Greco', 'Niguarda - Cà Granda', 'Bicocca', 'Vle Monza', 'Adriano', 'Parco Lambro - Cimiano', 'Padova', 'Loreto', 'Buenos Aires - Venezia', 'Città Studi', 'Lambrate', 'Parco Forlanini - Ortica', 'Corsica', 'XXII Marzo', 'Porta Romana', 'Umbria - Molise', 'Ortomercato', 'Mecenate', 'Parco Monlué - Ponte Lambro', 'Triulzo Superiore', 'Rogoredo', 'Chiaravalle', 'Lodi - Corvetto', 'Scalo Romana', 'Ex OM - Morivione', 'Ripamonti', 'Quintosole', 'Ronchetto delle Rane', 'Gratosoglio - Ticinello', 'Stadera', 'Tibaldi', 'Navigli', 'S. Cristoforo', 'Barona', 'Cantalupa', 'Ronchetto S/N', 'Giambellino', 'Tortona', 'Washington', 'Bande Nere', 'Lorenteggio', 'Muggiano', 'Baggio', 'Forze Armate', 'Selinunte', 'De Angeli - Monte Rosa', 'Tre Torri', 'S. Siro', 'Quarto Cagnino', 'Quinto Romano', 'Figino', 'Trenno', 'Gallaratese', 'QT 8', 'Portello', 'Pagano', 'Sarpi', 'Ghisolfa', 'Villapizzone', 'Maggiore - Musocco', 'Cascina Triulza - Expo', 'Sacco', 'Stephenson', 'Quarto Oggiaro', 'Bovisa', 'Farini', 'Dergano', 'Affori', 'Bovisasca', 'Comasina', 'Bruzzano', 'Parco Nord', 'Parco delle Abbazie', 'Parco dei Navigli', 'Parco Agricolo Sud', 'Parco Bosco in Città']\n",
      "Gli ID di quartiere a Milano sono 88\n",
      "Bad records: 0 su 2136906\n"
     ]
    }
   ],
   "source": [
    "quartieri = [] # quanti e quali quartieri abbiamo?\n",
    "missingRecords = 0 \n",
    "for item in data:\n",
    "    if 'Nil' in item:        \n",
    "        if item['Nil'] not  in quartieri:\n",
    "            quartieri.append(item['Nil'])\n",
    "    else:\n",
    "        missingRecords += 1\n",
    "\n",
    "print (quartieri)\n",
    "print ('Gli ID di quartiere a Milano sono ' + str(len(quartieri)))\n",
    "print ('Bad records: ' + str(missingRecords) + ' su ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
      "Anni nel dataset: 20\n",
      "Bad records: 0 su 2136906\n"
     ]
    }
   ],
   "source": [
    "anni = [] # quanti e quali anni abbiamo?\n",
    "missingRecords = 0 \n",
    "for item in data:\n",
    "    if 'Anno' in item:        \n",
    "        if item['Anno'] not  in anni:\n",
    "            anni.append(item['Anno'])\n",
    "    else:\n",
    "        missingRecords += 1\n",
    "\n",
    "print (anni)\n",
    "print ('Anni nel dataset: ' + str(len(anni)))\n",
    "print ('Bad records: ' + str(missingRecords) + ' su ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo che la popolazione corrisponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395274\n",
      "275818\n"
     ]
    }
   ],
   "source": [
    "pop = 0\n",
    "nonItaliani = 0\n",
    "for item in data:\n",
    "    if item['Anno'] == '2018':\n",
    "        pop += int(item['Residenti'])\n",
    "        if item['Cittadinanza'] != 'Italia':\n",
    "            nonItaliani += int(item['Residenti'])\n",
    "print(pop)\n",
    "print (nonItaliani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cinese, Rep. Popolare', 'Egitto', 'Etiopia', 'Filippine', 'Italia', 'Sri Lanka (ex Ceylon)', 'Togo', 'Perù', 'Francia', 'Libano', 'Mauritius', 'Regno Unito', 'India', 'Spagna', \"Stati Uniti D'America\", 'Israele', 'Turchia', 'Grecia', 'Marocco', 'Portogallo', 'Germania', 'Giappone', 'El Salvador', 'Iran', 'Bulgaria', 'Ecuador', 'Croazia', 'Bosnia-Erzegovina', 'ex Jugoslavia', 'Albania', 'Siria', 'Argentina', 'Cuba', 'Georgia', 'Dominicana, Rep.', 'Svizzera', 'Colombia', 'Eritrea', 'Romania', 'Russa, Federazione', 'Somalia', 'Ucraina', 'Venezuela', 'Belgio', 'Danimarca', 'Irlanda', 'Ungheria', 'Brasile', 'Svezia', 'Australia', 'Austria', 'Estonia', 'Dominica', 'Taiwan (ex Formosa)', 'Tunisia', 'Capo Verde', 'Ghana', 'Kenya', 'Lituania', 'Nigeria', 'Norvegia', 'Paesi Bassi', 'Ruanda', 'Thailandia', 'Giordania', 'Finlandia', 'Cile', 'Indonesia', 'Canada', 'Haiti', 'Nepal', 'Pakistan', 'Messico', 'Polonia', 'Corea del Sud, Rep.', 'Bolivia', 'Uruguay', 'Costa Rica', 'Senegal', 'Lussemburgo', 'Giamaica', 'Tanzania', 'Libia', 'Slovenia', 'Seychelles', 'Malta', 'San Marino', 'Ceca, Rep.', 'Sudan', 'Sud Africa', 'Nuova Zelanda', 'Iraq', 'Apolide', 'Slovacchia', 'Bangladesh', 'Kazakistan', \"Costa D'Avorio\", 'Camerun', 'Algeria', 'Benin (ex Dahomey)', 'Malaysia', 'Altre nazioni o n.d.', 'Guatemala', 'Uzbekistan', 'Islanda', 'Burkina Faso (ex Alto Volta)', 'Paraguay', 'Zambia', 'Angola', 'Guinea', 'Gambia', 'Panama', 'Congo', 'Cipro', 'Sierra Leone', 'Vietnam', 'Yemen', 'Lettonia', 'Uganda', 'Afghanistan', 'Singapore', 'Trinidad e Tobago', 'Burundi', 'Madagascar', 'Azerbaigian', 'Congo Rep. Dem (ex Zaire)', 'Arabia Saudita', 'Macedonia, ex Rep. Jugoslava', 'Nicaragua', 'Gabon', 'Myanmar (ex Birmania)', 'Liberia', 'Mozambico', 'Antigua e Barbuda', 'Moldova', 'Armenia', 'Mauritania', 'Bhutan', 'Mali', 'Suriname', 'Territori Autonomia Palestinese', 'Honduras', 'Zimbabwe (ex Rhodesia)', 'Andorra', 'Monaco', 'Gibuti', 'Niger', 'Ciad', 'Guinea Bissau', 'Bahamas', 'Mongolia', 'Barbados', 'Bielorussia', 'Kuwait', 'Turkmenistan', 'Corea del Nord, Rep. Pop. Dem', 'Centrafricana, Rep.', 'Tagikistan', 'Belize', 'Liechtenstein', 'Sao Tomè e Principe', 'Laos', 'Cambogia', 'Kirghizistan', 'Serbia', 'Montenegro', 'Kosovo', 'Maldive', 'Saint Kitts e Nevis', 'Malawi', 'Swaziland', 'Lesotho', 'Oman', 'Guinea Equatoriale', 'Sud Sudan, Repubblica del', 'Timor Orientale']\n",
      "Le nazionalità presenti a Milano sono 176\n",
      "Bad records: 0 su 2136906\n"
     ]
    }
   ],
   "source": [
    "nazioni = [] # quante e quali nazionalità abbiamo?\n",
    "missingRecords = 0 \n",
    "for item in data:\n",
    "    if 'Cittadinanza' in item:        \n",
    "        if item['Cittadinanza'] not  in nazioni:\n",
    "            nazioni.append(item['Cittadinanza'])\n",
    "    else:\n",
    "        missingRecords += 1\n",
    "\n",
    "print (nazioni)\n",
    "print ('Le nazionalità presenti a Milano sono ' + str(len(nazioni)))\n",
    "print ('Bad records: ' + str(missingRecords) + ' su ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo pre-aggregare i dati. \n",
    "\n",
    "Per la mappa 1 (vedere [il wiki](https://github.com/spaghetti-open-data/-milano-melting-pot/wiki/2.-Metodologia-e-workflow)), vogliamo sapere quante persone abbiamo di ciascuna nazionalità, in ciascun quartiere e in ciascun anno. Qundi dobbiamo sommare la variabile `'Residenti'` per ciascuna tripla di valori sulle variabili `'Anno'`, `'Nil'` e `'Cittadinanza'`. Inoltre, non dobbiamo perderci il genere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mi faccio un dizionario di liste di dizionari; questo serve a rendere il codice più efficiente. Questo mi servirà per generare i files JSON delle prime tre mappe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedData = {}\n",
    "for quartiere in quartieri:\n",
    "    nestedData[quartiere] = []\n",
    "    for item in data:\n",
    "        if item['Nil'] == quartiere:\n",
    "            nestedData[quartiere].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31858\n"
     ]
    }
   ],
   "source": [
    "test = nestedData['Duomo']\n",
    "\n",
    "print (len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++ Duomo\n",
      "++++++ Brera\n",
      "++++++ Giardini Porta Venezia\n",
      "++++++ Guastalla\n",
      "++++++ Vigentina\n",
      "++++++ Ticinese\n",
      "++++++ Magenta - S. Vittore\n",
      "++++++ Parco Sempione\n",
      "++++++ Garibaldi Repubblica\n",
      "++++++ Centrale\n",
      "++++++ Isola\n",
      "++++++ Maciachini - Maggiolina\n",
      "++++++ Greco\n",
      "++++++ Niguarda - Cà Granda\n",
      "++++++ Bicocca\n",
      "++++++ Vle Monza\n",
      "++++++ Adriano\n",
      "++++++ Parco Lambro - Cimiano\n",
      "++++++ Padova\n",
      "++++++ Loreto\n",
      "++++++ Buenos Aires - Venezia\n",
      "++++++ Città Studi\n",
      "++++++ Lambrate\n",
      "++++++ Parco Forlanini - Ortica\n",
      "++++++ Corsica\n",
      "++++++ XXII Marzo\n",
      "++++++ Porta Romana\n",
      "++++++ Umbria - Molise\n",
      "++++++ Ortomercato\n",
      "++++++ Mecenate\n",
      "++++++ Parco Monlué - Ponte Lambro\n",
      "++++++ Triulzo Superiore\n",
      "++++++ Rogoredo\n",
      "++++++ Chiaravalle\n",
      "++++++ Lodi - Corvetto\n",
      "++++++ Scalo Romana\n",
      "++++++ Ex OM - Morivione\n",
      "++++++ Ripamonti\n",
      "++++++ Quintosole\n",
      "++++++ Ronchetto delle Rane\n",
      "++++++ Gratosoglio - Ticinello\n",
      "++++++ Stadera\n",
      "++++++ Tibaldi\n",
      "++++++ Navigli\n",
      "++++++ S. Cristoforo\n",
      "++++++ Barona\n",
      "++++++ Cantalupa\n",
      "++++++ Ronchetto S/N\n",
      "++++++ Giambellino\n",
      "++++++ Tortona\n",
      "++++++ Washington\n",
      "++++++ Bande Nere\n",
      "++++++ Lorenteggio\n",
      "++++++ Muggiano\n",
      "++++++ Baggio\n",
      "++++++ Forze Armate\n",
      "++++++ Selinunte\n",
      "++++++ De Angeli - Monte Rosa\n",
      "++++++ Tre Torri\n",
      "++++++ S. Siro\n",
      "++++++ Quarto Cagnino\n",
      "++++++ Quinto Romano\n",
      "++++++ Figino\n",
      "++++++ Trenno\n",
      "++++++ Gallaratese\n",
      "++++++ QT 8\n",
      "++++++ Portello\n",
      "++++++ Pagano\n",
      "++++++ Sarpi\n",
      "++++++ Ghisolfa\n",
      "++++++ Villapizzone\n",
      "++++++ Maggiore - Musocco\n",
      "++++++ Cascina Triulza - Expo\n",
      "++++++ Sacco\n",
      "++++++ Stephenson\n",
      "++++++ Quarto Oggiaro\n",
      "++++++ Bovisa\n",
      "++++++ Farini\n",
      "++++++ Dergano\n",
      "++++++ Affori\n",
      "++++++ Bovisasca\n",
      "++++++ Comasina\n",
      "++++++ Bruzzano\n",
      "++++++ Parco Nord\n",
      "++++++ Parco delle Abbazie\n",
      "++++++ Parco dei Navigli\n",
      "++++++ Parco Agricolo Sud\n",
      "++++++ Parco Bosco in Città\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Cinese, Rep. Popolare', 'Maschi': 14, 'Femmine': 11}\n",
      "309760\n",
      "Completato in 0:23:40.832843\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start_script = datetime.datetime.now()\n",
    "summedData = []\n",
    "for key in nestedData:\n",
    "    y = nestedData[key]\n",
    "    print('++++++ '+ str(key))\n",
    "    for anno in anni:\n",
    "        # print (anno)\n",
    "        for nazione in nazioni: \n",
    "            triple = {'Anno': anno, 'Nil': key, 'Cittadinanza': nazione, 'Maschi': 0, 'Femmine': 0}\n",
    "            for item in y:\n",
    "                now = item ['Anno']\n",
    "                people = item  ['Cittadinanza']\n",
    "                if anno == now and people == nazione:\n",
    "                    if item['Genere'] == 'Femmine':\n",
    "                        triple['Femmine'] += int(item ['Residenti'])\n",
    "                    else: \n",
    "                        triple['Maschi'] += int(item['Residenti'])\n",
    "            summedData.append(triple)\n",
    "\n",
    "print (summedData[0])\n",
    "print (len(summedData))\n",
    "\n",
    "end_script = datetime.datetime.now()\n",
    "print ('Completato in ' + str(end_script-start_script))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo per vedere se funziona bene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Cinese, Rep. Popolare', 'Maschi': 14, 'Femmine': 11}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Egitto', 'Maschi': 40, 'Femmine': 12}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Etiopia', 'Maschi': 9, 'Femmine': 22}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Filippine', 'Maschi': 97, 'Femmine': 205}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Italia', 'Maschi': 8653, 'Femmine': 8948}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Sri Lanka (ex Ceylon)', 'Maschi': 91, 'Femmine': 76}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Togo', 'Maschi': 0, 'Femmine': 2}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Perù', 'Maschi': 21, 'Femmine': 57}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Francia', 'Maschi': 100, 'Femmine': 91}\n",
      "{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Libano', 'Maschi': 21, 'Femmine': 10}\n",
      "309760\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(summedData[i])\n",
    "print (len(summedData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sembra di sì. La variabile `Eta` è sparita; ora i residenti di tutte le età sono aggregati in un unico record per ciascuna nazionalità, quartiere e anno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora voglio aggregare le nazionalità per regioni geografiche. In prima approssimazione, manteniamo la classificazione del progetto [Brussels Melting Pot](http://brussels-diversity.jetpack.ai/), e usiamo i nomi inglesi delle categorie per mantenere la comparabilità con esso. Questa prevede due livelli di aggregazione: regioni e continenti. Per prima cosa costruisco liste dei paesi che appartengono a ciascuna regione. Le regioni sono:\n",
    "\n",
    "* EU 15 ([definizione](https://stats.oecd.org/glossary/detail.asp?ID=6805)), esclusa l'Italia\n",
    "* EU N13, cioè i 28 paesi dell'UE meno i 15 di EU 15 \n",
    "* Non-EU Europe (questo non c'è nel progetto Bruxelles)\n",
    "* North Africa \n",
    "* Sub-Saharian Africa\n",
    "* Eastern Asia ([definizione](https://en.wikipedia.org/wiki/East_Asia))\n",
    "* Southern Asia\n",
    "* Southeast Asia (non c'è nel progetto di Bruxelles, se i numeri sono piccoli lo elimino dopo)\n",
    "* Western Asia\n",
    "* North America\n",
    "* South America\n",
    "* Oceania\n",
    "* Other (residuo)\n",
    "\n",
    "I continenti sono:\n",
    "\n",
    "* Europe\n",
    "* Asia\n",
    "* Africa\n",
    "* North America\n",
    "* South America\n",
    "* Oceania\n",
    "\n",
    "(il residuo è omesso)\n",
    "\n",
    "Scrivo questa aggregazione come una funzione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_country(country):\n",
    "    '''\n",
    "    returns a tuple with the region and continent of country\n",
    "    '''\n",
    "    # elenco nazionalità, attribuite alla regione\n",
    "    \n",
    "    eu15 = ['Francia', 'Regno Unito', 'Spagna', 'Grecia', 'Portogallo', 'Germania', 'Belgio', 'Danimarca', 'Irlanda', \n",
    "        'Svezia', 'Austria', 'Paesi Bassi', 'Finlandia', 'Lussemburgo', 'Andorra', 'Monaco', 'Liechtenstein']\n",
    "    eun13 = ['Bulgaria', 'Croazia', 'Romania', 'Ungheria', 'Estonia', 'Lituania', 'Polonia', 'Slovenia', 'Ceca, Rep.', \n",
    "         'Slovacchia', 'Lettonia', 'Malta', 'Cipro']\n",
    "    otheu = ['Bosnia-Erzegovina', 'ex Jugoslavia', 'Albania', 'Georgia', 'Svizzera', 'Russa, Federazione', 'Ucraina', \n",
    "         'Norvegia', 'Moldova', 'Armenia', 'Islanda', 'Bielorussia', 'Azerbaigian', 'Kosovo', 'Serbia', 'Montenegro', \n",
    "         'Macedonia, ex Rep. Jugoslava']\n",
    "    noraf = ['Egitto', 'Marocco', 'Tunisia', 'Libia', 'Algeria']\n",
    "    ssaf = ['Etiopia', 'Togo', 'Mauritius', 'Eritrea', 'Somalia', 'Capo Verde', 'Ghana', 'Kenya', 'Nigeria', 'Ruanda', \n",
    "        'Senegal', 'Tanzania', 'Seychelles', 'Sudan', 'Sud Africa', \"Costa D'Avorio\", 'Camerun', \n",
    "        'Benin (ex Dahomey)', 'Burkina Faso (ex Alto Volta)', 'Zambia', 'Angola', 'Guinea', 'Gambia', 'Congo', \n",
    "        'Sierra Leone', 'Uganda', 'Burundi', 'Madagascar', 'Congo Rep. Dem (ex Zaire)', 'Gabon', 'Liberia', \n",
    "        'Mozambico', 'Mauritania', 'Mali', 'Zimbabwe (ex Rhodesia)', 'Gibuti', 'Niger', 'Ciad', 'Guinea Bissau', \n",
    "        'Centrafricana, Rep.', 'Sao Tomè e Principe', 'Malawi', 'Swaziland', 'Lesotho', 'Guinea Equatoriale', \n",
    "         'Sud Sudan, Repubblica del']\n",
    "    eastas = ['Cinese, Rep. Popolare', 'Giappone', 'Taiwan (ex Formosa)', 'Corea del Sud, Rep.', \n",
    "          'Corea del Nord, Rep. Pop. Dem', 'Mongolia']\n",
    "    southas = ['India', 'Bangladesh', 'Afghanistan', 'Bhutan', 'Maldive', 'Pakistan', 'Sri Lanka (ex Ceylon)', 'Nepal']\n",
    "    seas = ['Indonesia', 'Malaysia', 'Vietnam', 'Singapore', 'Myanmar (ex Birmania)', 'Laos', 'Cambogia', \n",
    "        'Timor Orientale', 'Thailandia', 'Filippine']\n",
    "    westas = ['Libano', 'Israele', 'Turchia', 'Iran', 'Siria', 'Iraq', 'Kazakistan', 'Uzbekistan', 'Yemen', \n",
    "          'Territori Autonomia Palestinese', 'Kuwait', 'Turkmenistan', 'Tagikistan', 'Kirghizistan', 'Oman']\n",
    "    noram = [\"Stati Uniti D'America\", 'Canada', 'Giordania', 'Arabia Saudita']\n",
    "    soam = ['Messico', 'Perù', 'El Salvador', 'Ecuador', 'Argentina', 'Cuba', 'Dominicana, Rep.', 'Colombia', \n",
    "        'Venezuela', 'Brasile', 'Dominica', 'Cile', 'Haiti', 'Bolivia', 'Uruguay', 'Costa Rica', 'Giamaica', \n",
    "       'Guatemala', 'Panama', 'Trinidad e Tobago', 'Nicaragua', 'Antigua e Barbuda', 'Suriname', 'Honduras', \n",
    "       'Bahamas', 'Barbados', 'Belize', 'Saint Kitts e Nevis', 'Paraguay']\n",
    "    ocean = ['Australia', 'Nuova Zelanda']\n",
    "    other = ['Altre nazioni o n.d.', 'Apolide']\n",
    "\n",
    "\n",
    "    if country in eu15:\n",
    "        location = ('eu15', 'europa')\n",
    "    elif country in eun13:\n",
    "        location = ('eun13', 'europa')\n",
    "    elif country in otheu:\n",
    "        location = ('otheu', 'europa')\n",
    "    elif country in noraf:\n",
    "        location = ('noraf', 'africa')\n",
    "    elif country in ssaf:\n",
    "        location = ('ssaf', 'africa')\n",
    "    elif country in eastas:\n",
    "        location = ('eastas', 'asia')\n",
    "    elif country in southas:\n",
    "        location = ('southas', 'asia')\n",
    "    elif country in seas:\n",
    "        location = ('seas', 'asia')\n",
    "    elif country in westas:\n",
    "        location = ('westas', 'asia')\n",
    "    elif country in noram:\n",
    "        location = ('noram', 'north america')\n",
    "    elif country in soam:\n",
    "        location = ('soam', 'south america')\n",
    "    elif country in ocean:\n",
    "        location = ('ocean', 'oceania')\n",
    "    elif country in other:\n",
    "        location = ('other','')\n",
    "    else: \n",
    "        location = ('ita', 'italia')\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimangono fuori dalle 176 nazionalità \n",
    "solo Italia e San Marino, che in effetti contano come indigeni a Milano. \n",
    "\n",
    "Aggiungo regioni e continenti al dataset, usando la mia funzione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Cinese, Rep. Popolare', 'Maschi': 14, 'Femmine': 11, 'regione': 'eastas', 'continente': 'asia'}, {'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Egitto', 'Maschi': 40, 'Femmine': 12, 'regione': 'noraf', 'continente': 'africa'}, {'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Etiopia', 'Maschi': 9, 'Femmine': 22, 'regione': 'ssaf', 'continente': 'africa'}, {'Anno': '1999', 'Nil': 'Duomo', 'Cittadinanza': 'Filippine', 'Maschi': 97, 'Femmine': 205, 'regione': 'seas', 'continente': 'asia'}]\n",
      "309760\n"
     ]
    }
   ],
   "source": [
    "for item in summedData:\n",
    "    nazione = item['Cittadinanza']\n",
    "    item['regione'] = assign_country(nazione)[0]\n",
    "    item['continente'] = assign_country(nazione)[1]\n",
    "\n",
    "print (summedData[:4]) ## just for testing  \n",
    "print (len(summedData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo è un primo risultato. Lo salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open (dirPath + 'summedData.json', 'w') as jsonfile:\n",
    "    json.dump(summedData, jsonfile)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produco un file per la mappa 1, con la struttura descritta [qui](https://github.com/spaghetti-open-data/-milano-melting-pot/issues/3#issue-451241445)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "[{'Anno': '1999', 'Nil': 'Duomo', 'eastas M': 38, 'eastas F': 33, 'eastas Tot': 71}, {'Anno': '1999', 'Nil': 'Duomo', 'noraf M': 68, 'noraf F': 34, 'noraf Tot': 102}, {'Anno': '1999', 'Nil': 'Duomo', 'ssaf M': 48, 'ssaf F': 93, 'ssaf Tot': 141}, {'Anno': '1999', 'Nil': 'Duomo', 'seas M': 100, 'seas F': 209, 'seas Tot': 309}]\n",
      "Completato in 0:11:34.105823\n"
     ]
    }
   ],
   "source": [
    "start_script = datetime.datetime.now()\n",
    "map1Data = []\n",
    "regio = ['eastas', 'noraf', 'ssaf', 'seas', 'ita', 'southas', 'soam', 'eu15', 'westas', 'noram', 'eun13', \n",
    "         'otheu', 'ocean', 'other']\n",
    "for anno in anni:\n",
    "    print (anno)\n",
    "    for quartiere in quartieri:\n",
    "        # print (quartiere)\n",
    "        for regione in regio: \n",
    "            newrecord ={'Anno': anno, 'Nil': quartiere}\n",
    "            for item in summedData: \n",
    "                if item['Anno'] == anno and item['Nil'] == quartiere and item['regione'] == regione:\n",
    "                    newkey = item['regione'] + ' M'\n",
    "                    if newkey not in newrecord:\n",
    "                        newrecord[newkey] = item['Maschi']\n",
    "                        newrecord[item['regione'] + ' F'] = item['Femmine']\n",
    "                        newrecord[item['regione'] + ' Tot'] = item['Maschi'] + item['Femmine']\n",
    "                    else:\n",
    "                        newrecord[item['regione'] + ' M'] += item['Maschi']\n",
    "                        newrecord[item['regione'] + ' F'] += item['Femmine']\n",
    "                        newrecord[item['regione'] + ' Tot'] += (item['Maschi'] + item['Femmine'])      \n",
    "            map1Data.append(newrecord)\n",
    "\n",
    "print(map1Data[:4])\n",
    "\n",
    "end_script = datetime.datetime.now()\n",
    "print ('Completato in ' + str(end_script-start_script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24640\n"
     ]
    }
   ],
   "source": [
    "print(len(map1Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, salviamo in json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dirPath2 = '/Users/albertocottica/Documents/GitHub/-milano-melting-pot/dati/'\n",
    "with open (dirPath2 + 'map1Data.json', 'w') as outfile:\n",
    "    json.dump(map1Data, outfile)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per facilitare il lavoro agli sviluppatori web, mi scrivo anche un dataset \"compatto\", con un record per regione, quartiere e anno. In questo modo le 176 nazionalità vengono compattate in 15 regioni. Mantengo l'informazione su Maschi e Femmine, che può servire, ma aggiungo per ciascuna regione un campo `Tot`, la somma di maschi + femmine. **Devo controllare con Tommaso, Cesare e Marco che questo formato gli funzioni. Se non gli funziona, cancello la cella**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "[{'Anno': '1999', 'Nil': 'Duomo', 'eastas M': 38, 'eastas F': 33, 'eastas Tot': 71, 'noraf M': 68, 'noraf F': 34, 'noraf Tot': 102, 'ssaf M': 48, 'ssaf F': 93, 'ssaf Tot': 141, 'seas M': 100, 'seas F': 209, 'seas Tot': 309, 'ita M': 8654, 'ita F': 8948, 'ita Tot': 17602, 'southas M': 112, 'southas F': 86, 'southas Tot': 198, 'soam M': 67, 'soam F': 193, 'soam Tot': 260, 'eu15 M': 336, 'eu15 F': 327, 'eu15 Tot': 663, 'westas M': 68, 'westas F': 56, 'westas Tot': 124, 'noram M': 73, 'noram F': 74, 'noram Tot': 147, 'eun13 M': 17, 'eun13 F': 39, 'eun13 Tot': 56, 'otheu M': 66, 'otheu F': 68, 'otheu Tot': 134, 'ocean M': 3, 'ocean F': 2, 'ocean Tot': 5, 'other M': 4, 'other F': 1, 'other Tot': 5}, {'Anno': '1999', 'Nil': 'Brera', 'eastas M': 51, 'eastas F': 54, 'eastas Tot': 105, 'noraf M': 66, 'noraf F': 28, 'noraf Tot': 94, 'ssaf M': 77, 'ssaf F': 104, 'ssaf Tot': 181, 'seas M': 78, 'seas F': 173, 'seas Tot': 251, 'ita M': 8179, 'ita F': 9344, 'ita Tot': 17523, 'southas M': 89, 'southas F': 58, 'southas Tot': 147, 'soam M': 73, 'soam F': 170, 'soam Tot': 243, 'eu15 M': 319, 'eu15 F': 341, 'eu15 Tot': 660, 'westas M': 24, 'westas F': 14, 'westas Tot': 38, 'noram M': 85, 'noram F': 78, 'noram Tot': 163, 'eun13 M': 18, 'eun13 F': 23, 'eun13 Tot': 41, 'otheu M': 82, 'otheu F': 84, 'otheu Tot': 166, 'ocean M': 3, 'ocean F': 5, 'ocean Tot': 8, 'other M': 2, 'other F': 1, 'other Tot': 3}, {'Anno': '1999', 'Nil': 'Giardini Porta Venezia', 'eastas M': 0, 'eastas F': 0, 'eastas Tot': 0, 'noraf M': 0, 'noraf F': 0, 'noraf Tot': 0, 'ssaf M': 0, 'ssaf F': 2, 'ssaf Tot': 2, 'seas M': 2, 'seas F': 4, 'seas Tot': 6, 'ita M': 17, 'ita F': 15, 'ita Tot': 32, 'southas M': 2, 'southas F': 1, 'southas Tot': 3, 'soam M': 1, 'soam F': 0, 'soam Tot': 1, 'eu15 M': 4, 'eu15 F': 4, 'eu15 Tot': 8, 'westas M': 0, 'westas F': 0, 'westas Tot': 0, 'noram M': 1, 'noram F': 0, 'noram Tot': 1, 'eun13 M': 0, 'eun13 F': 0, 'eun13 Tot': 0, 'otheu M': 0, 'otheu F': 0, 'otheu Tot': 0, 'ocean M': 0, 'ocean F': 0, 'ocean Tot': 0, 'other M': 0, 'other F': 0, 'other Tot': 0}, {'Anno': '1999', 'Nil': 'Guastalla', 'eastas M': 39, 'eastas F': 34, 'eastas Tot': 73, 'noraf M': 66, 'noraf F': 37, 'noraf Tot': 103, 'ssaf M': 48, 'ssaf F': 93, 'ssaf Tot': 141, 'seas M': 101, 'seas F': 197, 'seas Tot': 298, 'ita M': 6835, 'ita F': 7926, 'ita Tot': 14761, 'southas M': 110, 'southas F': 75, 'southas Tot': 185, 'soam M': 68, 'soam F': 185, 'soam Tot': 253, 'eu15 M': 176, 'eu15 F': 245, 'eu15 Tot': 421, 'westas M': 61, 'westas F': 47, 'westas Tot': 108, 'noram M': 41, 'noram F': 44, 'noram Tot': 85, 'eun13 M': 22, 'eun13 F': 29, 'eun13 Tot': 51, 'otheu M': 80, 'otheu F': 92, 'otheu Tot': 172, 'ocean M': 3, 'ocean F': 0, 'ocean Tot': 3, 'other M': 1, 'other F': 1, 'other Tot': 2}]\n",
      "44\n",
      "1760\n"
     ]
    }
   ],
   "source": [
    "compactData = []\n",
    "for anno in anni:\n",
    "    print (anno)\n",
    "    for quartiere in quartieri:\n",
    "        # print (quartiere)\n",
    "        newrecord ={'Anno': anno, 'Nil': quartiere}\n",
    "        for item in summedData: \n",
    "            if item['Anno'] == anno and item['Nil'] == quartiere:\n",
    "                newkey = item['regione'] + ' M'\n",
    "                if newkey not in newrecord:\n",
    "                    newrecord[newkey] = item['Maschi']\n",
    "                    newrecord[item['regione'] + ' F'] = item['Femmine']\n",
    "                    newrecord[item['regione'] + ' Tot'] = item['Maschi'] + item['Femmine']\n",
    "                else:\n",
    "                    newrecord[item['regione'] + ' M'] += item['Maschi']\n",
    "                    newrecord[item['regione'] + ' F'] += item['Femmine']\n",
    "                    newrecord[item['regione'] + ' Tot'] += (item['Maschi'] + item['Femmine'])      \n",
    "        compactData.append(newrecord)\n",
    "\n",
    "print(compactData[:4])\n",
    "print(len(compactData[0]))\n",
    "print(len(compactData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voglio compattare ancora di più, come richiesto da Marco Scarselli. Faccio due cose:\n",
    "\n",
    "* elimino il breakdown tra maschi e femmine.\n",
    "* sostituisco il `Nil` con l'identificativo `IdNil` (si trova nel file di dati grezzi).\n",
    "\n",
    "Per prima cosa, costruisco un dizionario che mappa da `Nil` a `IdNil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Duomo': '1', 'Brera': '2', 'Giardini Porta Venezia': '3', 'Guastalla': '4', 'Vigentina': '5', 'Ticinese': '6', 'Magenta - S. Vittore': '7', 'Parco Sempione': '8', 'Garibaldi Repubblica': '9', 'Centrale': '10', 'Isola': '11', 'Maciachini - Maggiolina': '12', 'Greco': '13', 'Niguarda - Cà Granda': '14', 'Bicocca': '15', 'Vle Monza': '16', 'Adriano': '17', 'Parco Lambro - Cimiano': '18', 'Padova': '19', 'Loreto': '20', 'Buenos Aires - Venezia': '21', 'Città Studi': '22', 'Lambrate': '23', 'Parco Forlanini - Ortica': '24', 'Corsica': '25', 'XXII Marzo': '26', 'Porta Romana': '27', 'Umbria - Molise': '28', 'Ortomercato': '29', 'Mecenate': '30', 'Parco Monlué - Ponte Lambro': '31', 'Triulzo Superiore': '32', 'Rogoredo': '33', 'Chiaravalle': '34', 'Lodi - Corvetto': '35', 'Scalo Romana': '36', 'Ex OM - Morivione': '37', 'Ripamonti': '38', 'Quintosole': '39', 'Ronchetto delle Rane': '40', 'Gratosoglio - Ticinello': '41', 'Stadera': '42', 'Tibaldi': '43', 'Navigli': '44', 'S. Cristoforo': '45', 'Barona': '46', 'Cantalupa': '47', 'Ronchetto S/N': '48', 'Giambellino': '49', 'Tortona': '50', 'Washington': '51', 'Bande Nere': '52', 'Lorenteggio': '53', 'Muggiano': '54', 'Baggio': '55', 'Forze Armate': '56', 'Selinunte': '57', 'De Angeli - Monte Rosa': '58', 'Tre Torri': '59', 'S. Siro': '60', 'Quarto Cagnino': '61', 'Quinto Romano': '62', 'Figino': '63', 'Trenno': '64', 'Gallaratese': '65', 'QT 8': '66', 'Portello': '67', 'Pagano': '68', 'Sarpi': '69', 'Ghisolfa': '70', 'Villapizzone': '71', 'Maggiore - Musocco': '72', 'Cascina Triulza - Expo': '73', 'Sacco': '74', 'Stephenson': '75', 'Quarto Oggiaro': '76', 'Bovisa': '77', 'Farini': '78', 'Dergano': '79', 'Affori': '80', 'Bovisasca': '81', 'Comasina': '82', 'Bruzzano': '83', 'Parco Nord': '84', 'Parco delle Abbazie': '85', 'Parco dei Navigli': '86', 'Parco Agricolo Sud': '87', 'Parco Bosco in Città': '88'}\n"
     ]
    }
   ],
   "source": [
    "dictMap = {}\n",
    "for item in data:\n",
    "    key = item['Nil']\n",
    "    if key not in dictMap:\n",
    "        dictMap[key] = item['IdNil']\n",
    "print(dictMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguo le trasformazioni su `compactData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760\n",
      "{'Anno': '1999', 'IdNil': '1', 'eastas': 71, 'noraf': 102, 'ssaf': 141, 'seas': 309, 'ita': 17602, 'southas': 198, 'soam': 260, 'eu15': 663, 'westas': 124, 'noram': 147, 'eun13': 56, 'otheu': 134, 'ocean': 5, 'other': 5}\n"
     ]
    }
   ],
   "source": [
    "ultraCompact = []\n",
    "for item in compactData:\n",
    "    newitem ={'Anno': item['Anno'], 'IdNil': dictMap[item['Nil']]}\n",
    "    for regione in regio:\n",
    "        newitem[regione] = item[regione + ' Tot']\n",
    "    ultraCompact.append(newitem)\n",
    "print(len(ultraCompact))\n",
    "print(ultraCompact[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvo anche questo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dirPath2 = '/Users/albertocottica/Documents/GitHub/-milano-melting-pot/dati/'\n",
    "with open (dirPath2 + 'map1Data_ultracompact.json', 'w') as outfile:\n",
    "    json.dump(ultraCompact, outfile)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
